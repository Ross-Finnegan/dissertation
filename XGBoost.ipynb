{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "#!nvidia-smi\n",
    "\n",
    "import numpy as np; print('numpy Version:', np.__version__)\n",
    "import pandas as pd; print('pandas Version:', pd.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter;\n",
    "import xgboost as xgb; print('XGBoost Version:', xgb.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = pd.read_csv('SQL_extracted datasets/Formatted_data/FINALlanghill_mir_extractready_revised_7pm_mir_aligned_20210711.csv', sep=',')\n",
    "print(data_cleaned.shape) #Shape is 417657 rows and 18 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data - Basic split\n",
    "80% Train, 10% Validation, 10% Test <br> Could also split by time (e.g. train 2013-2018, test 2018-2021) - Could randomize the data and split: most likely choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into 3 - train validation and test\n",
    "#Once model trained, test the model using best interation of predictions\n",
    "#For comparison of predicted vs observed\n",
    "#Plots of accuracy\n",
    "#Alternative split method which will randomly split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.1\n",
    "X = data_cleaned.iloc[:, 14:1074]\n",
    "y = data_cleaned['weeklyave_FI']\n",
    "\n",
    "X_test, X_train, y_test, y_train =train_test_split(X,y, train_size=0.1, random_state = 11)\n",
    "#_train, X_valid, y_train, y_valid = train_test_split(X_rem,y_rem, train_size=0.8)\n",
    "# check dimensions\n",
    "print('X_train: ', X_train.shape,  'y_train: ', y_train.shape)\n",
    "#print('X_validation', X_valid.shape, 'y_validation: ', y_valid.shape)\n",
    "print('X_test', X_test.shape, 'y_test: ', y_test.shape)\n",
    "\n",
    "\n",
    "# check the proportions\n",
    "total = X_train.shape[0]  + X_test.shape[0]\n",
    "print('X_train proportion:', X_train.shape[0] / total)\n",
    "#print('X_validation proportion:', X_valid.shape[0] / total)\n",
    "print('X_test proportion:', X_test.shape[0] / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "#dvalidation = xgb.DMatrix(X_valid, label=y_valid)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "## gbtree\n",
    "# instantiate params\n",
    "params = {}\n",
    "\n",
    "# general params\n",
    "general_params = {'silent': 1, 'booster' : 'gbtree'}\n",
    "params.update(general_params)\n",
    "\n",
    "# booster params\n",
    "n_gpus = 3\n",
    "booster_params = {}\n",
    "\n",
    "if n_gpus != 0:\n",
    "    booster_params['tree_method'] = 'gpu_hist'\n",
    "    booster_params['n_gpus'] = n_gpus\n",
    "params.update(booster_params)\n",
    "\n",
    "# learning task params\n",
    "#Check XGBoost manual for full parameter descriptions\n",
    "learning_task_params = {'eval_metric': 'rmse', 'objective': 'reg:linear'}\n",
    "\n",
    "\n",
    "params.update(learning_task_params)\n",
    "print(params)\n",
    "\n",
    "from xgboost import cv\n",
    "\n",
    "xgb_cv = cv(dtrain = dtrain, params = params, nfold = 10, early_stopping_rounds = 10, metrics = \"rmse\")\n",
    "xgb_cv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training settings\n",
    "Could set rounds to 10,000 with an early stopper. If no change in 25 loops can stop and return to best iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evallist = [(dvalidation, 'validation'), (dtrain, 'train')]\n",
    "#watchlist = [(dvalidation,'valid'), (dtrain, 'train')]\n",
    "watchlist = [ (dtrain, 'train')]\n",
    "\n",
    "#evals_result = {'valid':{}, 'train':{}}\n",
    "evals_result = {'train':{}}\n",
    "\n",
    "num_round = 208\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Train model\n",
    "bst = xgb.train(params,\n",
    "               dtrain,\n",
    "               num_round,\n",
    "               watchlist,\n",
    "               evals_result = evals_result,\n",
    "               early_stopping_rounds = 15,\n",
    "               verbose_eval = 100\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest, ntree_limit =  bst.best_ntree_limit)\n",
    "#preds\n",
    "import scipy as scipy\n",
    "slope, intercept, r, p, stderr = scipy.stats.linregress(y_test, preds)\n",
    "line = f'Regression line: y={intercept:.2f}+{slope:.2f}x, r={r:.2f}'\n",
    "line\n",
    "'Regression line: y=18.78+0.58x, r=0.73'\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(y_test,preds, linewidth=0, marker='s', label='Data points', alpha = 0.15)\n",
    "ax.plot(y_test, intercept + slope * y_test, label=line)\n",
    "\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.legend(facecolor='white')\n",
    "plt.show()\n",
    "#plt.figure(figsize=(8,6))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "ax.hist(y_test, bins=100, alpha=0.3, label=\"Actual\", color = \"green\")\n",
    "ax.hist(preds, bins=100, alpha=0.35, label=\"Predicted\", color = \"purple\")\n",
    "\n",
    "ax.set_xlabel(\"Feed intake (kg/day)\", size=14)\n",
    "ax.set_ylabel(\"Frequency\", size=14)\n",
    "ax.legend(facecolor='white')\n",
    "\n",
    "plt.title(\"Distribution of Actual vs Predicted Feed Intake (FI-0 PM)\")\n",
    "plt.savefig('XGBDist2.png', dpi=250)\n",
    "\n",
    "#plt.savefig(\"overlapping_histograms_with_matplotlib_Python.png\")\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(y_test, bins=100, alpha=0.3, label=\"Actual\", color = \"green\")\n",
    "plt.hist(preds, bins=100, alpha=0.35, label=\"Predicted\", color = \"purple\")\n",
    "\n",
    "plt.xlabel(\"Feed intake (kg/day)\", size=14)\n",
    "plt.ylabel(\"Frequency\", size=14)\n",
    "plt.title(\"Distribution of Actual vs Predicted \")\n",
    "plt.legend(loc='upper right')\n",
    "#plt.savefig(\"overlapping_histograms_with_matplotlib_Python.png\")\n",
    "\n",
    "y_test.shape\n",
    "preds.shape\n",
    "from scipy.stats import pearsonr\n",
    "# calculate Pearson's correlation\n",
    "corr, p = pearsonr(y_test, preds)\n",
    "print('Pearsons correlation: %.3f' % corr)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# calculate coeff of determination\n",
    "r2 = r2_score(y_test, preds)\n",
    "print('Coeff of determination: %.3f' % r2)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print('RMSE of prediction: %.3f' % rmse)\n",
    "from xgboost import cv\n",
    "\n",
    "r,p = pearsonr(preds, y_test)\n",
    "import scipy as sp\n",
    "m_slope, c_intercept, r_value, p_value, std_err = sp.stats.linregress(y_test, preds)\n",
    "m, c = np.polyfit(y_test, preds, 1\n",
    "                 )\n",
    "std_err\n",
    "xgb.plot_importance(\"bst\")\n",
    "xgb.plot_importance(\"bst\", importance_type = \"cover\")\n",
    "xgb.plot_importance(\"bst\", importance_type = \"gain\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Export\n",
    "Create dataframe with just actual values, predicted values and EAR_TAG. this will be used for genetic analysis of phenotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data = {'Actual': y_test,\n",
    "              'Preds': preds \n",
    "              }\n",
    "df = pd.DataFrame(export_data, columns = ['Actual', 'Preds'])\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
